# -*- coding: utf-8 -*-
"""CS412-HW4-ZeniaMazhar.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nbLcoDnktGraA-wNwFXToXeClXoDcFJq

# CS412 - Machine Learning - 2024-2025
## Homework 4


## Dataset
[**CelebA**](https://mmlab.ie.cuhk.edu.hk/projects/CelebA.html) is a large-scale face attributes dataset with more than 200K celebrity images, each with 40 attribute annotations. The images in this dataset cover large pose variations and background clutter.

**Download the data from Sucourse and upload it to your Google Drive. You must upload both CelebA30k.zip and CelebA30k.csv to your Drive without renaming them.
CelebA30k is a smaller version of the original CelebA dataset, containing 30,000 images, to reduce computation time during training.
Split the data as follows: reserve 10% of the samples for validation and 10% for testing (i.e., 3,000 samples). The remaining 80% should be used for training.
Important: The test set (3,000 samples) must be used only for final evaluation and not for model selection or hyperparameter tuning.**

## Task

Build a gender classifier using the PyTorch library and a pretrained VGG-16 model on the CelebA dataset. Your goal is to complete the given code without changing the base architecture of VGG-16 (other than replacing the classification head suitable for binary classification).

You will:
- Freeze the convolutional layers of the pretrained model  
- Replace the classifier head with your own binary classification layer  
- Fine-tune the model on the given subset of CelebA  
- Experiment with different values of learning rate, batch size, and number of epochs  

**Dataset:** CelebA  
**Model:** VGG-16
**Library:** PyTorch  

---

## Software

You will use the PyTorch framework for this homework. PyTorch offers flexible tools for deep learning, including pretrained models, custom training loops, and GPU acceleration.

##1) Initialize

*   First make a copy of the notebook given to you as a starter.

*   Make sure you change your runtime to GPU.

## 2) Load training dataset
"""

# load data
from google.colab import drive
drive.mount('/content/drive/')

# Commented out IPython magic to ensure Python compatibility.
# Import necessary libraries

import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from PIL import Image
from sklearn.model_selection import train_test_split

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torchvision import datasets, models, transforms

from torch.utils.data import DataLoader, Dataset

# For reproducibility
np.random.seed(42)
torch.manual_seed(42)

# Set device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# %matplotlib inline

data = pd.read_csv('/content/drive/My Drive/CelebA30k.csv') # enter the file path on your drive for the csv file
data.head()

gender_data = data[['filename', 'Male']].copy()
gender_data.head()

# This will extract the contents of the zip file into a folder named data
# Do not extract the zip into your google drive

!unzip "/content/drive/My Drive/CelebA30k.zip" -d "/content/data"

first_image_path = os.path.join("/content/data/CelebA30k/", gender_data.loc[0,"filename"])
img = Image.open(first_image_path)

img

"""# Starting here, you’ll need to fill in the code cells on your own.

##3) Visualizing/Understanding the dataset

- Display five random images together with their labels

- Display statistics about the dataset, such as distribution of labels, etc.
"""

# Sample 5 random rows from the gender_data DataFrame
sampled_data = gender_data.sample(5)

# Set up the plot
plt.figure(figsize=(15, 5))

# Loop through the sampled rows
for i, (_, row) in enumerate(sampled_data.iterrows()):
    img_path = os.path.join("/content/data/CelebA30k/", row["filename"])
    img = Image.open(img_path)

    # Convert label
    gender = "Male" if row["Male"] == 1 else "Female"

    # Plot
    plt.subplot(1, 5, i + 1)
    plt.imshow(img)
    plt.title(gender)
    plt.axis("off")

plt.tight_layout()
plt.show()

# Count of each gender
label_counts = gender_data["Male"].value_counts().sort_index()

# Convert index to labels for readability
label_counts.index = ["Female (-1)", "Male (1)"]

# Display raw counts
print("Label Distribution:\n", label_counts)

# Plot the distribution
label_counts.plot(kind='bar', color=['pink', 'lightblue'])
plt.title("Gender Distribution in CelebA30k")
plt.ylabel("Number of Images")
plt.xticks(rotation=0)
plt.show()

# to display any null values

print(data.isnull().sum())

# to see which attributes are seen in how many images

attribute_counts = (data.iloc[:, 1:] == 1).sum().sort_values(ascending=False)
attribute_counts.plot(kind='bar', figsize=(15,5), color='skyblue')
plt.title("Positive Attribute Count")
plt.ylabel("Number of Images")
plt.xticks(rotation=90)
plt.show()

#to see how much each attribute correlates with each gender

import seaborn as sns

#compute correlations of each attribute with gender
correlations = data.drop(columns=["filename"]).corr()

# get just the correlation of each attribute with the 'Male' column
gender_corr = correlations["Male"].drop("Male")  # drop self-correlation

# sort for clearer view
gender_corr_sorted = gender_corr.sort_values()

# plot as horizontal bar chart
plt.figure(figsize=(10, 8))
sns.barplot(x=gender_corr_sorted.values, y=gender_corr_sorted.index, palette="coolwarm")

plt.title("Correlation of Attributes with Gender (Male)")
plt.xlabel("Correlation with 'Male' (1 = male, -1 = female)")
plt.grid(axis='x')
plt.axvline(0, color='gray', linestyle='--')
plt.tight_layout()
plt.show()

"""##4) Split the dataset as train (80%), validation (10%) and test (10%) set."""

train_df, temp_df = train_test_split(data, test_size=0.2, random_state=42, stratify=data["Male"]) #temp_df is the val+test (20%)
val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42, stratify=temp_df["Male"]) #split temp_df into half val and half test

"""## 5) Preparing the Data

In this section, you will implement the dataset loading and preprocessing pipeline using PyTorch.

You are provided with:
- `train_df` and `val_df` dataframes, which include two columns: `filename`  and `Male`
- A directory path (`/content/data/CelebA30k`) that contains the image files

Your task is to:
1. Define appropriate transformations using `torchvision.transforms` for the training and validation sets
   - Resize images to 224x224
   - Convert them to tensors
   - Normalize them (you can use `[0.5], [0.5]` for simplicity)
   - Add data augmentation (e.g., horizontal flip) for training
2. Write a custom `Dataset` class that reads images and labels from the dataframe
3. Create `DataLoader` objects for training and validation with an appropriate batch size
"""

import torchvision.transforms as transforms

# Training transforms (with augmentation)
train_transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.RandomHorizontalFlip(),  # simple augmentation
    transforms.ToTensor(),
    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])
])

# Validation transforms (no augmentation)
val_transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])
])

class Dataset(Dataset):
    def __init__(self, dataframe, root_dir, transform=None):
        self.dataframe = dataframe.reset_index(drop=True)
        self.root_dir = root_dir
        self.transform = transform

    def __len__(self):
        return len(self.dataframe)

    def __getitem__(self, idx):
        img_name = self.dataframe.loc[idx, "filename"]
        img_path = os.path.join(self.root_dir, img_name)
        image = Image.open(img_path).convert("RGB")

        label = self.dataframe.loc[idx, "Male"]
        label = 1 if label == 1 else 0  # Convert -1 to 0 for binary classification

        if self.transform:
            image = self.transform(image)

        return image, label

root_dir = "/content/data/CelebA30k"

train_dataset = Dataset(train_df, root_dir, transform=train_transform)
val_dataset = Dataset(val_df, root_dir, transform=val_transform)

from torch.utils.data import DataLoader

batch_size = 64

train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)
val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)

"""## 6) Transfer Learning with VGG-16

We will create the base model from the well-known VGG-16 architecture available in torchvision.models. This model is pretrained on the ImageNet dataset, a large dataset containing 1.4 million images and 1,000 object categories.

In transfer learning, we use the convolutional base of the pretrained model as a feature extractor. However, we do not use the last fully connected layer, which is specific to ImageNet’s 1,000-class classification task. Instead, we will:
- Keep the convolutional layers (which extract general visual features)
- Remove the final classification layer
- Replace it with our own binary classifier (for gender classification)

This is a common strategy because the layer before the original classification head (the so-called "bottleneck layer") retains high-level, general features useful for a wide range of vision tasks.

### Instructions

You should:
1. Load the VGG-16 model with pretrained=True from torchvision.models.
2. Replace the classifier head with a new sequence of layers suitable for binary classification:
   - The final output should have one neuron (output dimension = 1).
   - Do not apply a Sigmoid activation after the output layer, because the loss function you will use internally applies it.

"""

vgg16 = models.vgg16(pretrained=True)

vgg16.classifier = nn.Sequential(
    nn.Linear(25088, 4096),
    nn.ReLU(inplace=True),
    nn.Dropout(0.5),
    nn.Linear(4096, 1024),
    nn.ReLU(inplace=True),
    nn.Dropout(0.5),
    nn.Linear(1024, 1)  # binary output, no sigmoid
)

"""## 7) Fine-Tuning and Training the Model

Now that your VGG-16 base model is set up with a new binary classification head, it’s time to fine-tune and train it using your `train_loader` and evaluate it using your `val_loader`.

You will:
- Experiment with two different fine-tuning strategies:
  - Freeze all convolutional layers and train only the classifier head.
  - Unfreeze the last convolutional block and the classifier head, and train them.
- Choose an appropriate optimizer (e.g., `torch.optim.SGD` or `Adam`).
- Use `nn.BCEWithLogitsLoss()` as the loss function.
  - Note: Since you are using nn.BCEWithLogitsLoss(), your model's output should not have a Sigmoid activation. This loss function applies the Sigmoid operation internally.
- Implement a training loop that includes validation after each epoch.
- Monitor both training and validation performance across epochs.

Hyperparameter Tuning:
- Try two different learning rates: 0.001 and 0.0001.
- The number of training epochs must be fixed to 10.
- You may adjust batch size depending on available GPU memory, but typically 32 or 64 is suggested.

Finally, you should report:
- Final training and validation accuracy for each configuration.
- A plot showing training and validation loss across epochs.
- A brief discussion comparing the results for different fine-tuning strategies and learning rates, identifying which combination performed best and why.

"""

# Function to train the model with specified learning rate and fine-tuning strategy
def train_model(learning_rate, fine_tuning_strategy):
    # Reset model (to avoid training on the same model multiple times)
    vgg16 = models.vgg16(pretrained=True)

    # Modify the classifier for binary classification (1 output)
    vgg16.classifier[6] = nn.Linear(4096, 1)  # 1 output unit for binary classification

    # Apply fine-tuning strategy
    if fine_tuning_strategy == "freeze_all":
        # Freeze all convolutional layers
        for param in vgg16.features.parameters():
            param.requires_grad = False

        # Define optimizer to only update the classifier parameters
        optimizer = torch.optim.Adam(vgg16.classifier.parameters(), lr=learning_rate)

    elif fine_tuning_strategy == "unfreeze_last_block":
        # Unfreeze the last convolutional block (block 5 in VGG-16)
        for param in vgg16.features[24:].parameters():
            param.requires_grad = True

        # Define optimizer to update both the last convolutional block and the classifier head
        optimizer = torch.optim.Adam(vgg16.parameters(), lr=learning_rate)

    # Loss function
    criterion = nn.BCEWithLogitsLoss()

    # Lists to track loss and accuracy for plotting
    train_losses = []
    val_losses = []
    train_accuracies = []
    val_accuracies = []

    # Training loop
    for epoch in range(1):
        vgg16.train()
        running_loss = 0.0
        correct = 0
        total = 0

        for inputs, labels in train_loader:
            optimizer.zero_grad()

            outputs = vgg16(inputs)
            loss = criterion(outputs, labels.unsqueeze(1).float())  # Convert labels to float for BCE loss
            loss.backward()
            optimizer.step()

            running_loss += loss.item()
            preds = torch.round(torch.sigmoid(outputs))  # Convert logits to binary predictions
            correct += (preds == labels.unsqueeze(1)).sum().item()
            total += labels.size(0)

        train_loss = running_loss / len(train_loader)
        train_accuracy = correct / total
        train_losses.append(train_loss)
        train_accuracies.append(train_accuracy)

        print(f'Epoch {epoch+1}, Loss: {train_loss}, Accuracy: {train_accuracy}')

        # Validation loop
        vgg16.eval()
        val_loss = 0.0
        correct = 0
        total = 0
        with torch.no_grad():
            for inputs, labels in val_loader:
                outputs = vgg16(inputs)
                loss = criterion(outputs, labels.unsqueeze(1).float())
                val_loss += loss.item()
                preds = torch.round(torch.sigmoid(outputs))
                correct += (preds == labels.unsqueeze(1)).sum().item()
                total += labels.size(0)

        val_loss_avg = val_loss / len(val_loader)
        val_accuracy = correct / total
        val_losses.append(val_loss_avg)
        val_accuracies.append(val_accuracy)

        print(f'Validation Loss: {val_loss_avg}, Validation Accuracy: {val_accuracy}')

    return train_losses, val_losses, train_accuracies, val_accuracies

# Function to plot loss curves
def plot_loss_curves(train_losses, val_losses, learning_rate, fine_tuning_strategy):
    plt.figure(figsize=(10, 6))
    plt.plot(range(1, len(train_losses) + 1), train_losses, label='Train Loss', color='blue')
    plt.plot(range(1, len(val_losses) + 1), val_losses, label='Validation Loss', color='red')
    plt.title(f'Training and Validation Loss (LR={learning_rate}, Strategy={fine_tuning_strategy})')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.grid(True)
    plt.show()

# Experiment with learning rate 0.001 and fine-tuning strategies
for lr in [0.001, 0.0001]:
    for strategy in ["freeze_all", "unfreeze_last_block"]:
        print(f"\nTraining with learning rate {lr} and strategy: {strategy}")

        # Train the model and get metrics
        train_losses, val_losses, train_accuracies, val_accuracies = train_model(learning_rate=lr, fine_tuning_strategy=strategy)

        # Final accuracy report
        final_train_accuracy = train_accuracies[-1]
        final_val_accuracy = val_accuracies[-1]
        print(f"Final Training Accuracy: {final_train_accuracy}")
        print(f"Final Validation Accuracy: {final_val_accuracy}")

        # Plot the loss curves
        plot_loss_curves(train_losses, val_losses, lr, strategy)

# Define learning rates and fine-tuning strategies
learning_rates = [0.001, 0.0001]
fine_tuning_strategies = ["freeze_all", "unfreeze_last_block"]

def run_experiment(lr, strategy):
    print(f"\nTraining with learning rate {lr} and strategy: {strategy}")

    # Train the model and get metrics
    train_losses, val_losses, train_accuracies, val_accuracies = train_model(learning_rate=lr, fine_tuning_strategy=strategy)

    # Final accuracy report
    final_train_accuracy = train_accuracies[-1]
    final_val_accuracy = val_accuracies[-1]
    print(f"Final Training Accuracy: {final_train_accuracy}")
    print(f"Final Validation Accuracy: {final_val_accuracy}")

    # Plot the loss curves
    plot_loss_curves(train_losses, val_losses, lr, strategy)

# Experiment with learning rate 0.001 and all strategies
run_experiment(lr=0.001, strategy="freeze_all")

# Experiment with learning rate 0.001 and all strategies
run_experiment(lr=0.001, strategy="unfreeze_last_block")

# Experiment with learning rate 0.0001 and all strategies
run_experiment(lr=0.0001, strategy="freeze_all")

# Experiment with learning rate 0.0001 and all strategies
run_experiment(lr=0.0001, strategy="unfreeze_last_block")

"""## 8) Test your classifier on Test set

- Use your model to predict the labels of the test set and report the final accuracy.
"""

